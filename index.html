<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="HumanDiT">
    <meta property="og:title" content="HumanDiT"/>
    <meta property="og:description"
          content="HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/video_t1.png"/>
    <meta property="og:image:width" content="2412"/>
    <meta property="og:image:height" content="1394"/>


    <meta name="twitter:title" content="EMO">
    <meta name="twitter:description"
          content="HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation.">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/video_t1.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="Image-to-Video">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>HumanDiT</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css2?family=Jost:wght@300;400;500&display=swap" rel="stylesheet">

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
</nav>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=qZvhvPcAAAAJ" target="_blank">Qijun Gan</a><sup>1</sup><sup>*</sup>,</span>
                        <span class="author-block">
                <a href="https://rayeren.github.io/" target="_blank">Yi Ren</a><sup>2</sup><sup>*</sup><sup>&dagger;</sup>,</span>
                        <span class="author-block">
                <a href="" target="_blank">Chen Zhang</a><sup>2</sup><sup>*</sup>,</span>
                        <span class="author-block">
                <a href="https://yerfor.github.io/" target="_blank">Zhenhui Ye</a><sup>1</sup>,</span>
                        <span class="author-block">
                <a href="" target="_blank">Pan Xie</a><sup>2</sup>,</span>
                        <span class="author-block">
                <a href="" target="_blank">Xiang Yin</a><sup>2</sup>,</span>
                        <span class="author-block">
                <a href="" target="_blank">Zehuan Yuan</a><sup>2</sup>,</span>
                        <span class="author-block">
                <a href="" target="_blank">Bingyue Peng</a><sup>2</sup>,</span>
                        <span class="author-block">
                <a href="https://person.zju.edu.cn/en/jkzhu" target="_blank">Jianke Zhu</a><sup>1</sup>
                  </span>
                    </div>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Zhejiang University,</span>
                        <span class="author-block"><sup>2</sup>ByteDance</span>
                    </div>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>*</sup>Equal contribution,</span>
                        <span class="author-block"><sup>&dagger;</sup>Project leader</span>
                    </div>
                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.04847" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <!--                <div style="display: flex; justify-content: center; align-items: center;">-->
                <!--                    <img src="content/images/framework/intro.png" alt="MY ALT TEXT" style="width: 80%; height: 80%;"/>-->
                <!--                </div>-->
                <div class="content has-text-justified">
                    <p style="font-size: 1.2em;">
                        Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. 
                        To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences.
                        Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences.
                        Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.
                    </p>
                </div>
            </div>
        </div>
        <!--                                            <video class="video-player" poster="" id="tree1" controls>-->
        <!--                    <source src="content/video/main_page.mp4" type="video/mp4">-->
        <!--                </video>-->
    </div>
</section>

<!-- Method -->
<section class="hero is-small">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title is-2">Method</h2>
            <div style="display: flex; justify-content: center; align-items: center;">
                <img src="content/framework_arxiv.png" alt="MY ALT TEXT" style="width: 70%; height: 70%;"/>
            </div>
            <div class="item">
                <h2 class="content has-text-justified">
                    <p style="font-size: 1.2em;">
                        The overview of HumanDiT.
                         HumanDiT focuses on generate videos from a single image using a pose-guided DiT model. A 3D VAE is employed to encode video segments into latent space. With 3D full attention, the initial frame (green border) serves as a noise-free prefix latent (green cube) for reference. The pose guider extracts body and background pose features, while the DiT-based denoising model renders the final pixel results. During inference, the keypoint-DiT model produces subsequent motions based on the pose of the first frame. With a guided pose sequence, the pose adapter transfers and refines poses via keypoint-DiT to animate the reference image.
                    </p>
                </h2>
                <div class="item">
                </div>
            </div>
</section>
<!-- End Method -->

<style>
    .gifImage:hover {
        opacity: 0.8;
        box-shadow: 0 0 5px rgba(0, 0, 0, 0.5);
        transform: scale(1.1);
    }

    .paused {
        animation-play-state: paused;
    }

</style>


<head>
    <title>place gif</title>
    <style>
        .gif-container {
            display: flex;
        }

        .gif {
            width: 660px; /* 设置 GIF 的宽度 */
            height: 400px; /* 设置 GIF 的高度 */
        }
    </style>
</head>


<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-3">Various Generated Videos</h2>
            <br></br>
<!--            <h3 class="title is-3">Singing</h3>-->
            <h2 class="content has-text-justified">
                <p style="font-size: 1.4em;"></p>
            </h2>

            <!-- Different garments -->
            <div style="display: flex; justify-content: center;align-items: center; height: 80px;">
                <h3 class="title is-4"> Pose Transfer</h3>
            </div>

            <style>
                .video-item {
                    display: flex;
                    justify-content: center;
                    align-items: center;
                }

                .video-item column {
                    width: 48%; /* 各占一半，留下一些间隔 */
                    object-fit: cover; /* 保持图片纵横比 */
                    margin: 0 10px;
                }


                .item-gif-container {
                    width: 48%; /* 占据父元素的80%宽度 */
                    margin: auto; /* 水平方向上创建相同的空白 */
                }
            </style>

            <style>
                .video-container {
                    display: flex;
                    justify-content: center;
                    flex-wrap: wrap;
                    /*margin: 0 10px;*/
                    width: 100%;
                }

                .description {
                    font-size: 1.2em;
                    text-align: justify;
                }

                .content-description {
                    margin-top: 10px; /* Add some space above the description */
                }
            </style>
            <style>
                .caption {
                    margin-top: 8px; /* Space between the video and its caption */
                }
            </style>

            <div class="content has-text-justified">
                <p style="font-size: 1.2em;">
                    By inputting a single character image and template pose video, our method can generate vocal avatar videos featuring not only pose-accurate rendering but also realistic body shapes.
                </p>
            </div>

            <div class="video-item">

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/pose_transfer/1_1.mp4" type="video/mp4">
                    </video>
                </div>

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/pose_transfer/3_1.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

            <div class="video-item">

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/pose_transfer/4_1.mp4" type="video/mp4">
                    </video>
                </div>

                    <div class="column">
                        <video class="video-player" poster="" id="tree2" controls>
                            <source src="content/pose_transfer/6_1.mp4" type="video/mp4">
                        </video>
                    </div>

            </div>

            <div class="video-item">

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/pose_transfer/7_1.mp4" type="video/mp4">
                    </video>
                </div>

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/pose_transfer/8_1.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

            <div style="display: flex; justify-content: center;align-items: center; height: 80px;">
                <h3 class="title is-4"> Video Continuation</h3>
            </div>

            <div class="content has-text-justified">
                <p style="font-size: 1.2em;">
                    Our model supports video continuation for single human image, generating diverse and realistic motions such as speeches and dancing.
                </p>
            </div>

            <div class="video-item">

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/video_continue/4.mp4" type="video/mp4">
                    </video>
                </div>

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/video_continue/2.mp4" type="video/mp4">
                    </video>
                </div>
            </div>

            <div class="video-item">

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/video_continue/3.mp4" type="video/mp4">
                    </video>
                </div>

                <div class="column">
                    <video class="video-player" poster="" id="tree2" controls>
                        <source src="content/video_continue/1.mp4" type="video/mp4">
                    </video>
                </div>
            </div>     
        </div>
    </div>
</section>
<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{gan2025humanditposeguideddiffusiontransformer,
            title={HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation}, 
            author={Qijun Gan and Yi Ren and Chen Zhang and Zhenhui Ye and Pan Xie and Xiang Yin and Zehuan Yuan and Bingyue Peng and Jianke Zhu},
            year={2025},
            eprint={2502.04847},
            archivePrefix={arXiv},
            primaryClass={cs.CV},
            url={https://arxiv.org/abs/2502.04847}, 
        }
      </code></pre>
    </div>
 </section>
 <!--End BibTex citation -->

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content" style="text-align: center;">
                    <p>
                        This project is intended solely for academic research and effect demonstration.
                    </p>

                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                            target="_blank">Template</a> which was adopted from the <a href="https://nerfies.github.io"
                                                                                       target="_blank">Nerfies</a> project
                        page.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<script src="static/js/script.js"></script>
<script>
    // 获取所有视频元素
    var videos = document.querySelectorAll('video');

    // 为每个视频添加播放事件监听器
    videos.forEach(function (video) {
        video.addEventListener('play', function () {
            // 当任何一个视频开始播放时，暂停其他所有视频
            videos.forEach(function (otherVideo) {
                if (otherVideo !== video) {
                    otherVideo.pause();
                }
            });
        }, false);
    });
</script>

<script>
    new BeforeAfter({
        id: '#example1'
    });
    new BeforeAfter({
        id: '#example2'
    });
    new BeforeAfter({
        id: '#example3'
    });
    new BeforeAfter({
        id: '#example4'
    });
    new BeforeAfter({
        id: '#example6'
    });
    new BeforeAfter({
        id: '#example7'
    });

</script>

<script>
    var gifImage = document.getElementById('gifImage');
    var isPaused = false;

    gifImage.addEventListener('mouseenter', function () {
        gifImage.src = gifImage.src;
        isPaused = true;
    });

    gifImage.addEventListener('mouseleave', function () {
        if (isPaused) {
            gifImage.src = gifImage.src;
            isPaused = false;
        }
    });
</script>

<script>
    bulmaCarousel.attach('#results-carousel11', {
        slidesToScroll: 1,
        slidesToShow: 2,
        infinite: true,
        autoplay: false,
    });
    bulmaCarousel.attach('#results-carousel22', {
        slidesToScroll: 1,
        slidesToShow: 1,
        infinite: true,
        autoplay: false,
    });
    bulmaCarousel.attach('#results-carousel44', {
        slidesToScroll: 1,
        slidesToShow: 2,
        infinite: false,
        autoplay: false,
    });
</script>

<script>
    document.getElementById('gifImage3').src = 'content/gifs/Item.gif';
    document.getElementById('gifImage1').src = 'content/gifs/s1.gif';
    document.getElementById('gifImage2').src = 'content/gifs/s2.gif';

    // 图片资源路径
    const images = [
        'content/teaser/t3.gif',
        'content/teaser/t4.gif',
        'content/teaser/t1.gif',
        'content/teaser/t2.gif'
    ];

    // 获取要插入图片的div
    const group1 = document.getElementById('group1');
    const group2 = document.getElementById('group2');

    // 创建并插入前两张图片
    for (let i = 0; i < 2; i++) {
        const img = document.createElement('img');
        img.src = images[i];
        img.loading = 'lazy';
        img.alt = '图片' + (i + 1);
        group1.appendChild(img);
    }

    // 创建并插入后两张图片
    for (let i = 2; i < images.length; i++) {
        const img = document.createElement('img');
        img.src = images[i];
        img.loading = 'lazy';
        img.alt = '图片' + (i + 1);
        group2.appendChild(img);
    }

</script>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->
<!--<script type="text/javascript">-->
<!--    var sc_project = 12948235;-->
<!--    var sc_invisible = 1;-->
<!--    var sc_security = "8cca28b8";-->
<!--</script>-->
<!--<script type="text/javascript"-->
<!--        src="https://www.statcounter.com/counter/counter.js" async></script>-->
<!--<noscript>-->
<!--    <div class="statcounter"><a title="Web Analytics"-->
<!--                                href="https://statcounter.com/" target="_blank"><img class="statcounter"-->
<!--                                                                                     src="https://c.statcounter.com/12948235/0/8cca28b8/1/"-->
<!--                                                                                     alt="Web Analytics"-->
<!--                                                                                     referrerPolicy="no-referrer-when-downgrade"></a>-->
<!--    </div>-->
<!--</noscript>-->
<script type="text/javascript">
var sc_project=13078958;
var sc_invisible=1;
var sc_security="aa19b24c";
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js"
async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img
class="statcounter"
src="https://c.statcounter.com/13078958/0/aa19b24c/1/"
alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->
</body>


</html>
